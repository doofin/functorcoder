package functorcoder.llm

import io.circe.generic.auto._

import io.circe.*
import io.circe.syntax.*

object openaiReq {

  /** https://platform.openai.com/docs/models/model-endpoint-compatibility
    *
    * All GPT-4o, GPT-4o-mini, GPT-4, and GPT-3.5 Turbo models and their dated releases. chatgpt-4o-latest dynamic
    * model. Fine-tuned versions of gpt-4o, gpt-4o-mini, gpt-4, and gpt-3.5-turbo.
    */
  object models {
    val gpt4o = "gpt-4o" // price is 2.5 per 1M tokens
    /** gpt-4o-mini is a smaller version of the GPT-4o
      */
    val gpt4oMini = "gpt-4o-mini" // price is 0.15 per 1M tokens
    val gpt4 = "gpt-4"
    val gpt35Turbo = "gpt-3.5-turbo"
    val o3mini = "o3-mini" // reasoning model,1.1 per 1M tokens
  }

  object roles {
    val user = "user" // user input in request
    val system = "system" // control messages in request
    val assistant = "assistant" // the response from the model
  }

  /** Represents a message in a conversation.
    *
    * @param role
    *   The role of the speaker. Must be either "user" or "system".
    * @param content
    *   The content of the message. This field is required.
    */
  case class Message(role: String, content: String)

  /** Represents the request body for interacting with the API.
    *
    * @param messages
    *   A list of messages comprising the conversation so far. This field is required.
    * @param model
    *   The ID of the model to use. This field is required. Refer to the model endpoint compatibility table for details
    *   on which models work with the Chat API.
    *
    * below are optional fields
    *
    * @param frequency_penalty
    *   Optional. A number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency
    *   in the text so far, decreasing the model's likelihood to repeat the same line verbatim. Defaults to None.
    * @param logit_bias
    *   Optional. A map that modifies the likelihood of specified tokens appearing in the completion. Accepts a JSON
    *   object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to
    *   100. The bias is added to the logits generated by the model prior to sampling. The exact effect varies per
    *   model. Defaults to None.
    * @param logprobs
    *   Optional. Whether to return log probabilities of the output tokens. If true, returns the log probabilities of
    *   each output token returned in the content of the message. Defaults to None.
    * @param top_logprobs
    *   Optional. An integer between 0 and 20 specifying the number of most likely tokens to return at each token
    *   position, each with an associated log probability. Must be set if `logprobs` is true. Defaults to None.
    * @param max_tokens
    *   Optional. The maximum number of tokens that can be generated in the chat completion. The total length of input
    *   tokens and generated tokens is limited by the model's context length. Defaults to None.
    * @param n
    *   Optional. How many chat completion choices to generate for each input message. Note that you will be charged
    *   based on the number of generated tokens across all choices. Defaults to None.
    * @param presence_penalty
    *   Optional. A number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the
    *   text so far, increasing the model's likelihood to talk about new topics. Defaults to None.
    * @param response_format
    *   Optional. An object specifying the format that the model must output. Compatible with GPT-4o, GPT-4o mini, GPT-4
    *   Turbo, and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106. Defaults to None.
    * @param seed
    *   Optional. If specified, the system will make a best effort to sample deterministically. Repeated requests with
    *   the same seed and parameters should return the same result. Determinism is not guaranteed. Defaults to None.
    * @param service_tier
    *   Optional. Specifies the latency tier to use for processing the request. If set to 'auto', the system will
    *   utilize scale tier credits until they are exhausted. If set to 'default', the request will be processed using
    *   the default service tier with a lower uptime SLA and no latency guarantee. When not set, the default behavior is
    *   'auto'. Defaults to None.
    * @param stop
    *   Optional. Up to 4 sequences where the API will stop generating further tokens. Defaults to None.
    * @param stream
    *   Optional. If set, partial message deltas will be sent as data-only server-sent events as they become available,
    *   with the stream terminated by a `data: [DONE]` message. Defaults to None.
    * @param stream_options
    *   Optional. Options for streaming response. Only set this when you set `stream` to true. Defaults to None.
    * @param temperature
    *   Optional. The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more
    *   random, while lower values like 0.2 will make it more focused and deterministic. Defaults to None.
    * @param top_p
    *   Optional. An alternative to sampling with temperature, called nucleus sampling, where the model considers the
    *   results of the tokens with top_p probability mass. Defaults to None.
    * @param tools
    *   Optional. A list of tools the model may call. Currently, only functions are supported as a tool. A max of 128
    *   functions are supported. Defaults to None.
    * @param tool_choice
    *   Optional. Controls which (if any) tool is called by the model. Defaults to `none` when no tools are present, and
    *   `auto` if tools are present. Defaults to None.
    * @param parallel_tool_calls
    *   Optional. Whether to enable parallel function calling during tool use. Defaults to None.
    * @param user
    *   Optional. A unique identifier representing your end-user, which can help monitor and detect abuse. Defaults to
    *   None.
    */
  case class OpenAiRequest(
      messages: Seq[Message],
      model: String
      // frequency_penalty: Option[Double] = None,
      // logit_bias: Option[Map[String, Int]] = None,
      // logprobs: Option[Boolean] = None,
      // top_logprobs: Option[Int] = None,
      // max_tokens: Option[Int] = None,
      // n: Option[Int] = None,
      // presence_penalty: Option[Double] = None,
      // response_format: Option[String] = None,
      // seed: Option[Int] = None,
      // service_tier: Option[String] = None,
      // stop: Option[Either[String, Seq[String]]] = None,
      // stream: Option[Boolean] = None,
      // stream_options: Option[String] = None,
      // temperature: Option[Double] = None,
      // top_p: Option[Double] = None,
      // tools: Option[String] = None,
      // tool_choice: Option[String] = None,
      // parallel_tool_calls: Option[Boolean] = None,
      // user: Option[String] = None
  ) {
    def toJson: String = this.asJson.noSpaces
  }

}
/* openAI API request
https://platform.openai.com/docs/api-reference/chat
'{
    "model": "gpt-4o",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Hello!"
      }
    ]
  }'
 */
